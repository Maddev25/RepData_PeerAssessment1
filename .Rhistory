table(isF, isJ)
newdt[isF & isJ, Special.Notes]
count(newdt[isF & isJ, Special.Notes])
library(quantmod)
install.packages(quantmod)
library(quantmod)
install.packages("quantmod")
library(quantmod)
amzn = getSymbols("AMZN",auto.assign=FALSE)
head(amzn)
sampleTimes = index(amzn)
sampleTimes
year(sampleTimes)
table(year(sampleTimes))
table(weekdays(sampleTimes))
table(year(sampleTimes), weekdays(sampleTimes))
addmargins(table(year(sampleTimes), weekdays(sampleTimes)))
q()
source('D:/DataScienceWorkspace/Getting_n_Cleaning_Data_week4_Assignment/run_analysis.R')
source('D:/DataScienceWorkspace/Getting_n_Cleaning_Data_week4_Assignment/run_analysis.R')
source('D:/DataScienceWorkspace/Getting_n_Cleaning_Data_week4_Assignment/run_analysis.R')
setwd("D:\\datascieneworkspace")
setwd("D:\datascieneworkspace")
setwd("D://datascieneworkspace")
getwd ()
source('D:/DataScienceWorkspace/Getting_n_Cleaning_Data_week4_Assignment/run_analysis.R')
source('D:/DataScienceWorkspace/Getting_n_Cleaning_Data_week4_Assignment/run_analysis.R')
source('D:/DataScienceWorkspace/Getting_n_Cleaning_Data_week4_Assignment/run_analysis.R')
source('D:/DataScienceWorkspace/Getting_n_Cleaning_Data_week4_Assignment/run_analysis.R')
source('D:/DataScienceWorkspace/Getting_n_Cleaning_Data_week4_Assignment/run_analysis.R')
source('D:/DataScienceWorkspace/Getting_n_Cleaning_Data_week4_Assignment/run_analysis.R')
source('D:/DataScienceWorkspace/Getting_n_Cleaning_Data_week4_Assignment/run_analysis.R')
source('D:/DataScienceWorkspace/Getting_n_Cleaning_Data_week4_Assignment/run_analysis.R')
fp <- file.path("./data", "UCI HAR Dataset")
fileSet <- list.files(path = fp , recursive = TRUE)
fileSet
source('D:/DataScienceWorkspace/Getting_n_Cleaning_Data_week4_Assignment/run_analysis.R')
q()
getwd()
source('D:/DataScienceWorkspace/Getting_n_Cleaning_Data_week4_Assignment/run_analysis.R')
source('D:/DataScienceWorkspace/Getting_n_Cleaning_Data_week4_Assignment/run_analysis.R')
source('D:/DataScienceWorkspace/Getting_n_Cleaning_Data_week4_Assignment/run_analysis.R')
source('D:/DataScienceWorkspace/Getting_n_Cleaning_Data_week4_Assignment/run_analysis.R')
source('D:/DataScienceWorkspace/Getting_n_Cleaning_Data_week4_Assignment/run_analysis.R')
source('D:/DataScienceWorkspace/Getting_n_Cleaning_Data_week4_Assignment/run_analysis.R')
head(X_merged)
source('D:/DataScienceWorkspace/Getting_n_Cleaning_Data_week4_Assignment/run_analysis.R')
X_merged
X_merged
X_merged <- X_merged[,msdVariables[,1]]
msdVariables <- vNames[grep("mean\\(\\)|std\\(\\)", vNames[,2]),]
X_merged <- X_merged[,msdVariables[,1]]
msdVariables
X_merged[,msdVariables[,1]]
library(dplyr)
X_merged[,msdVariables[,1]]
X_merged[, 1]
X_merged[,msdVariables[,1]]
msdVariables[,1]
X_merged[, msdVariables[,1]]
X_merged <- rbind(X_train, X_test)
X_merged <- X_merged[,msdVariables[,1]]
X_merged
colnames(Y_merged)
Y_merged <- rbind(Y_train, Y_test)
colnames(Y_merged)
Y_merged
Y_train
Y_test
Y_train  <- read.table(file.path(path= fp, "train" , "y_train.txt"), header = FALSE)
Y_train
source('D:/DataScienceWorkspace/Getting_n_Cleaning_Data_week4_Assignment/run_analysis.R')
colnames(Y_merged) <- "activity"
Y_merged$activitylabel <- factor(Y_merged$activity, labels = as.character(aLables[,2]))
Y_merged
source('D:/DataScienceWorkspace/Getting_n_Cleaning_Data_week4_Assignment/run_analysis.R')
colnames(X_merged) <- vNames[msdVariables[,1],2]
X_merged
source('D:/DataScienceWorkspace/Getting_n_Cleaning_Data_week4_Assignment/run_analysis.R')
source('D:/DataScienceWorkspace/Getting_n_Cleaning_Data_week4_Assignment/run_analysis.R')
source('D:/DataScienceWorkspace/Getting_n_Cleaning_Data_week4_Assignment/run_analysis.R')
source('D:/DataScienceWorkspace/Getting_n_Cleaning_Data_week4_Assignment/run_analysis.R')
source('D:/DataScienceWorkspace/Getting_n_Cleaning_Data_week4_Assignment/run_analysis.R')
source('D:/DataScienceWorkspace/Getting_n_Cleaning_Data_week4_Assignment/run_analysis.R')
library(knitr)
install.packages(knitr)
install.packages("knitr")
library(knitr)
knit2html("codebook.Rmd");
source('D:/DataScienceWorkspace/Getting_n_Cleaning_Data_week4_Assignment/run_analysis.R')
source('D:/DataScienceWorkspace/Getting_n_Cleaning_Data_week4_Assignment/run_analysis.R')
q()
source('D:/DataScienceWorkspace/Getting_n_Cleaning_Data_week4_Assignment/run_analysis.R')
names(newDSMean)
str(newDSMean)
quit()
library(swirl)
swirl()
swirl()
install_from_swirl("Explotary Data Analysis")
install_from_swirl("Exploratary Data Analysis")
install_from_swirl("Exploratory Data Analysis")
swirl()
head(pollution)
dim(pollution)
summary(pollution$pm25)
quantile(ppm)
boxplot(ppm, col = "blue")
abline(h=12)
hist(ppm, col="green")
rug(ppm)
low
high
hist(ppm,col="green", breaks = "100")
hist(ppm,col="green", breaks = 100)
rug
rug(ppm)
hist(ppm, col = "green")
abline(v=12, lwd=2)
abline(v=median(ppm), col = "magenta", lwd =4)
names(pollution)
reg<-table(pollution$region)
reg
barplot(reg, col="wheat", main = "Number of Counties in Each Region")
boxplot(pm25~region, data = pollution, col = 'red')
par(mfrow = c(2,1), mar= c(4,4,2,1))
east <- subset(pollution, region = "east")
east <- subset(pollution, region == "east")
head(east)
hist(east$pm25, col="green")
hist(subset(pollution, region == "west")$pm25, col="green")
pollution,plot(latitude, pm25)
pollution(plot(latitude, pm25))
plot()
plot(latitude, pm25)
plot(pollution(latitude, pm25))
with(pollution, plot(latitude, pm25))
abline(h=12, lwd=2, lty=2)
plot()
play()
asdaf
nxt()
plot(pollution$latitude, ppm, col=pollution$region)
abline(h=12, lwd=2, lty=2)
par(mfrow = c(1,2), mar= c(5,4,2,1))
west<-subset(pollution, region == "west")
plot(west$latitude, west$pm25, main = "West")
plot(east$latitude, east$pm25, main = "East")
?Devices
with(faithful, plot(eruptions, waiting))
title(main="Old faithful Geyser data")
title(main="Old Faithful Geyser data")
dev.cur()
pdf(file = "myplot.pdf")
with(faithful, plot(eruptions, waiting))
title(main="Old Faithful Geyser data")
dev.cur()
dev.off()
dev.cur()
with(faithful, plot(eruptions, waiting))
title(main="Old Faithful Geyser data")
dev.copy(png, fileset = "geyserplot.png")
dev.copy(png, file = "geyserplot.png")
dev.off()
bye()
swirl()
library(swirl)
swirl()
head(cars)
with(cars, plot(speed, dist))
text(mean(cars$speed), max(car$dist), "SWIRL rules!")
text(mean(cars$speed), max(cars$dist), "SWIRL rules!")
head(state)
table(state$region)
xyplot(Life.Exp ~ Income | region, state, c(4,1))
xyplot(Life.Exp ~ Income | region, data =state, layout = c(4,1))
xyplot(Life.Exp ~ Income | region, data =state, layout = c(2,2))
head(mpg)
dim(mpg)
mpg$model
table(mpg$model)
qplot(displ, hwy , data = mpg)
head(airquality)
range(ozone,na.rm = TRUE)
range(airquality$ozone,na.rm = TRUE)
range(airquality$Ozone,na.rm = TRUE)
hist(airquality$Ozone)
table(airquality$Month)
boxplot(Ozone ~ Month, data = airquality)
boxplot(Ozone ~ Month, data = airquality, xlab = "Month", ylab = "Ozone(ppb)", col.axis = "blue", col.lab = "red")
boxplot(Ozone ~ Month, airquality, xlab = "Month", ylab = "Ozone(ppb)", col.axis = "blue", col.lab = "red")
boxplot(Ozone ~ Month, airquality, xlab = "Month", ylab = "Ozone(ppb)", col.axis = "blue", col.lab = "red")
boxplot(Ozone~Month, airquality, xlab="Month", ylab="Ozone(ppb)",col.axis="blue",col.lab="red")
info()
skip()
bye()
getwd()
setwd("D:/DataScienceWorkspace/")
getwd()
ls
ls()
dir
dir()
dir()
cd Getting*
cd Getting_n_Cleaning_Data_week4_Assignment
dir  "\Getting_n_Cleaning_Data_week4_Assignment"
?unzip
unzip("exdata%2Fdata%2Fhousehold_power_consumption.zip", exdir = ".")
dir()
IHEC <- read.table("household_power_consumption.txt", skip = 1, sep = ";")
names(IHEC) <- c("Date","Time","Global_active_power","Global_reactive_power","Voltage","Global_intensity","Sub_metering_1","Sub_metering_2","Sub_metering_3")
subihec <- subset("IHEC", IHEC$Date = "01/02/2017" | "02/02/2017")
subihec <- subset("IHEC", IHEC$Date == "01/02/2017" | "02/02/2017")
subihec <- subset("IHEC", IHEC$Date == "01/02/2017" | IHEC$Date == "02/02/2017")
head(subihec)
ads <- subset("IHEC", IHEC$Date == "01/02/2017" | IHEC$Date == "02/02/2017")
head(IHEC)
ads <- subset("IHEC", IHEC$Date == "1/2/2017" | IHEC$Date == "2/2/2017")
head(ads)
rm ads
remove ads
rm(ads)
rm(subihec)
ads <- subset(IHEC, IHEC$Date == "1/2/2017" | IHEC$Date == "2/2/2017")
head(ads)
rm(ads)
actds <- subset(IHEC, IHEC$Date %in% c("1/1/2007", '2/2/2007'))
head(actds)
tail(actds)
range(actds$Date)
range(table(actds$Date))
table(actds$Date)
rm(actds)
subset(IHEC, IHEC$Date %in% c('1/2/2007', '2/2/2007'))
ds <- subset(IHEC, IHEC$Date %in% c('1/2/2007', '2/2/2007'))
summary(DS)
summary(ds)
hist(as.numeric(as.character(ds$Global_active_power)), col = red, xlab = "Global Active Power (kilowatts)")
dev.cur()
hist(ds$Global_active_power, col = red, xlab = "Global Active Power (kilowatts)")
hist(as.numeric(ds$Global_active_power), col = red, xlab = "Global Active Power (kilowatts)")
hist(as.numeric(as.character(ds$Global_active_power), col = red, xlab = "Global Active Power (kilowatts)")
hist(as.numeric(as.character(ds$Global_active_power)), col = red, xlab = "Global Active Power (kilowatts)")
hist(as.numeric(as.character(ds$Global_active_power)), col = "red", xlab = "Global Active Power (kilowatts)", main = "Global Active Power")
rm (ds)
source('D:/DataScienceWorkspace/ExData_Plotting1/plot1.R')
source('D:/DataScienceWorkspace/ExData_Plotting1/plot1.R')
source('D:/DataScienceWorkspace/ExData_Plotting1/plot1.R')
source('D:/DataScienceWorkspace/ExData_Plotting1/plot1.R')
?dev2bitmap
source('D:/DataScienceWorkspace/ExData_Plotting1/plot1.R')
source('D:/DataScienceWorkspace/ExData_Plotting1/plot1.R')
source('D:/DataScienceWorkspace/ExData_Plotting1/plot1.R')
source('D:/DataScienceWorkspace/ExData_Plotting1/plot1.R')
source('D:/DataScienceWorkspace/ExData_Plotting1/plot2.R')
source('D:/DataScienceWorkspace/ExData_Plotting1/plot2.R')
source('D:/DataScienceWorkspace/ExData_Plotting1/plot2.R')
ds$Date <- as.Date(ds$Date, format="%d/%m/%Y")
ds$Time <- strptime(ds$Time, format="%H:%M:%S")
ds[1:1440,"Time"] <- format(ds[1:1440,"Time"],"2007-02-01 %H:%M:%S")
ds[1441:2880,"Time"] <- format(ds[1441:2880,"Time"],"2007-02-02 %H:%M:%S")
head(ds)
ds <- subset(IHEC,IHEC$Date %in% c('1/2/2007', '2/2/2007'))
head(ds)
count(ds)
table(ds$Time)
range(table(ds$Time))
table(ds$Time)
summary(ds)
summary(ds)
rows(ds)
row(ds)
dim(ds)
subset(ds, ds$Time == "?")
ds$Date <- as.Date(ds$Date, format="%d/%m/%Y")
ds$Time <- strptime(ds$Time, format="%H:%M:%S")
head(ds)
tail(ds)
plot(ds$Time ~ ds$Global_active_power)
plot(ds$Time, ds$Global_active_power, type = "l")
plot(ds$Time, ds$Global_active_power, type = "l", xlab = "", ylab= "Global Active Power")
ds[1:1440,"Time"] <- format(ds[1:1440,"Time"],"2007-02-01 %H:%M:%S")
ds[1441:2880,"Time"] <- format(ds[1441:2880,"Time"],"2007-02-02 %H:%M:%S")
head(ds)
tail(ds)
plot(ds$Time, ds$Global_active_power, type = "l", xlab = "", ylab= "Global Active Power")
source('D:/DataScienceWorkspace/ExData_Plotting1/plot2.R')
source('D:/DataScienceWorkspace/ExData_Plotting1/plot2.R')
plot(ds$Time, ds$Sub_metering_1, type = "n" , ylab = "Energy Sub Metering", xlab="" )
with(ds, lines(Time, Sub_metering_1, col= "black"))
with(ds, lines(Time, Sub_metering_2, col= "blue"))
with(ds, lines(Time, Sub_metering_3, col= "red"))
plot(ds$Time, ds$Sub_metering_1, type = "n" , ylab = "Energy Sub Metering", xlab="" )
with(ds, lines(Time, Sub_metering_1, col= "black"))
with(ds, lines(Time, Sub_metering_2, col= "red"))
with(ds, lines(Time, Sub_metering_3, col= "blue"))
legend("topright", lty = 1, col = c)
legend("topright", lty = 1, col = c("black", "red", "blue"), legend = c("Sub_Metering_1", "Sub_Metering_2", "Sub_Metering)3"))
legend("topright", lty = 1, col = c("black", "red", "blue"), legend = c("Sub_Metering_1", "Sub_Metering_2", "Sub_Metering_3"))
legend("topright", lty = 1, col = c("black", "red", "blue"), legend = c("Sub_metering_1", "Sub_metering_2", "Sub_metering_3"))
source('D:/DataScienceWorkspace/ExData_Plotting1/plot3.R')
source('D:/DataScienceWorkspace/ExData_Plotting1/plot3.R')
source('D:/DataScienceWorkspace/ExData_Plotting1/plot3.R')
source('D:/DataScienceWorkspace/ExData_Plotting1/plot3.R')
source('D:/DataScienceWorkspace/ExData_Plotting1/plot3.R')
source('D:/DataScienceWorkspace/ExData_Plotting1/plot3.R')
source('D:/DataScienceWorkspace/ExData_Plotting1/plot3.R')
source('D:/DataScienceWorkspace/ExData_Plotting1/plot3.R')
source('D:/DataScienceWorkspace/ExData_Plotting1/plot3.R')
source('D:/DataScienceWorkspace/ExData_Plotting1/plot3.R')
source('D:/DataScienceWorkspace/ExData_Plotting1/plot4.R')
source('D:/DataScienceWorkspace/ExData_Plotting1/plot4.R')
source('D:/DataScienceWorkspace/ExData_Plotting1/plot4.R')
source('D:/DataScienceWorkspace/ExData_Plotting1/plot4.R')
source('D:/DataScienceWorkspace/ExData_Plotting1/plot4.R')
source('D:/DataScienceWorkspace/ExData_Plotting1/plot4.R')
source('D:/DataScienceWorkspace/ExData_Plotting1/plot4.R')
source('D:/DataScienceWorkspace/ExData_Plotting1/plot4.R')
source('D:/DataScienceWorkspace/ExData_Plotting1/plot4.R')
source('D:/DataScienceWorkspace/ExData_Plotting1/plot1.R')
source('D:/DataScienceWorkspace/ExData_Plotting1/plot2.R')
source('D:/DataScienceWorkspace/ExData_Plotting1/plot3.R')
source('D:/DataScienceWorkspace/ExData_Plotting1/plot4.R')
setwd("./RepData*")
setwd("./RepData_PeerAssessment1")
dir()
## Load the Data
if(!file.exists('activity.csv')) {
unzip('activity.zip')
}
actData <- read.csv("activity.csv")
summary(actData)
## Preprocess the Data for formatting the date fields
actData$date = as.Date(actData$date)
## Loadneeded R packages
library(dplyr)
## Calculate the total number of steps taken per day
s1 <- group_by (actData, date)
stepsperday <- summarize(s1, stepsTotal = sum(steps, na.rm = TRUE))
print(stepsperday)
## If you do not understand the difference between a histogram and a barplot, research the difference between them. Make a histogram of the total number of steps taken each day
hist(stepsperday$stepsTotal, main = "Histogram of Steps Per Day", xlab = "Total Steps per day")
## Calculate and report the mean and median of the total number of steps taken per day
medianStepsperday <- median(stepsperday$stepsTotal)
meanStepsperday <- mean(stepsperday$stepsTotal)
## Median Steps per day
print(medianStepsperday)
## Mean Steps per day
print(meanStepsperday)
## Make a time series plot (i.e. type = "l") of the 5-minute interval (x-axis) and the average number of steps taken, averaged across all days (y-axis)
s2 <- group_by (actData, interval)
avgstepsperInterval <- summarize(s2, avgs = mean(steps, na.rm = TRUE))
print(avgstepsperInterval)
plot(avgstepsperInterval$interval, avgstepsperInterval$avgs, type = "l", main = "Average Daily Activity Pattern", xlab = "Intervals", ylab = "Average Steps")
##Which 5-minute interval, on average across all the days in the dataset, contains the maximum number of steps?
maxStepsPerInterval <-avgstepsperInterval[which.max(avgstepsperInterval$avgs),]$interval
print(maxStepsPerInterval)
## Calculate and report the total number of missing values in the dataset (i.e. the total number of rows with NAs)
totMisValues <- sum(is.na(actData$steps))
print (totMisValues)
## Devise a strategy for filling in all of the missing values in the dataset. The strategy does not need to be sophisticated. For example, you could use the mean/median for that day, or the mean for that 5-minute interval, etc
### The Strategy to input the missing values would be mean for that day.
s3 <- group_by(actData, date)
avgStepsperDate <- summarize(s3,avgs = mean(steps, na.rm = TRUE))
print (avgStepsperDate)
## Create a new dataset that is equal to the original dataset but with the missing data filled in
## Make a histogram of the total number of steps taken each day and Calculate and report the mean and median total number of steps taken per day. Do these values differ from the estimates from the first part of the assignment? What is the impact of imputing missing data on the estimates of the total daily number of steps?
## Loadneeded R packages
library(dplyr)
## Calculate the total number of steps taken per day
s1 <- group_by (actData, date)
stepsperday <- summarize(s1, stepsTotal = sum(steps, na.rm = TRUE))
print(stepsperday)
## If you do not understand the difference between a histogram and a barplot, research the difference between them. Make a histogram of the total number of steps taken each day
hist(stepsperday$stepsTotal, main = "Histogram of Steps Per Day", xlab = "Total Steps per day")
## Calculate and report the mean and median of the total number of steps taken per day
medianStepsperday <- median(stepsperday$stepsTotal)
meanStepsperday <- mean(stepsperday$stepsTotal)
## Median Steps per day
print(medianStepsperday)
## Mean Steps per day
print(meanStepsperday)
## Calculate and report the total number of missing values in the dataset (i.e. the total number of rows with NAs)
totMisValues <- sum(is.na(actData$steps))
print (totMisValues)
## Devise a strategy for filling in all of the missing values in the dataset. The strategy does not need to be sophisticated. For example, you could use the mean/median for that day, or the mean for that 5-minute interval, etc
### The Strategy to input the missing values would be mean for that day.
s3 <- group_by(actData, date)
avgStepsperDate <- summarize(s3,avgs = mean(steps, na.rm = TRUE))
print (avgStepsperDate)
## Create a new dataset that is equal to the original dataset but with the missing data filled in
## Make a histogram of the total number of steps taken each day and Calculate and report the mean and median total number of steps taken per day. Do these values differ from the estimates from the first part of the assignment? What is the impact of imputing missing data on the estimates of the total daily number of steps?
avgStepsperDate
## Calculate and report the total number of missing values in the dataset (i.e. the total number of rows with NAs)
totMisValues <- sum(is.na(actData$steps))
print (totMisValues)
## Devise a strategy for filling in all of the missing values in the dataset. The strategy does not need to be sophisticated. For example, you could use the mean/median for that day, or the mean for that 5-minute interval, etc
## The Strategy to input the missing values would be mean for that interval and below function returns mean value for that interval
getMissingValue <- function(interval) {
avgstepsperInterval[avgstepsperInterval$interval == interval,]$steps
}
## Create a new dataset that is equal to the original dataset but with the missing data filled in
refData <- actData
for (i in 1:nrow(refData))
{
if(is.na(refdata[i,]$steps)) {
refData[i,]$steps <- getMissingValue(refData[i,]$interval)
}
}
## Calculate and report the total number of missing values in the dataset (i.e. the total number of rows with NAs)
totMisValues <- sum(is.na(actData$steps))
print (totMisValues)
## Devise a strategy for filling in all of the missing values in the dataset. The strategy does not need to be sophisticated. For example, you could use the mean/median for that day, or the mean for that 5-minute interval, etc
## The Strategy to input the missing values would be mean for that interval and below function returns mean value for that interval
getMissingValue <- function(interval) {
avgstepsperInterval[avgstepsperInterval$interval == interval,]$steps
}
## Create a new dataset that is equal to the original dataset but with the missing data filled in
refData <- actData
for (i in 1:nrow(refData))
{
if(is.na(refData[i,]$steps)) {
refData[i,]$steps <- getMissingValue(refData[i,]$interval)
}
}
## Calculate and report the total number of missing values in the dataset (i.e. the total number of rows with NAs)
totMisValues <- sum(is.na(actData$steps))
print (totMisValues)
## Devise a strategy for filling in all of the missing values in the dataset. The strategy does not need to be sophisticated. For example, you could use the mean/median for that day, or the mean for that 5-minute interval, etc
## The Strategy to input the missing values would be mean for that interval and below function returns mean value for that interval
getMissingValue <- function(interval) {
avgstepsperInterval[avgstepsperInterval$interval == interval,]$steps
}
## Create a new dataset that is equal to the original dataset but with the missing data filled in
refData <- actData
head(refData)
for (i in 1:nrow(refData))
{
if(is.na(refData[i,]$steps)) {
refData[i,]$steps <- getMissingValue(refData[i,]$interval)
}
}
## Calculate and report the total number of missing values in the dataset (i.e. the total number of rows with NAs)
totMisValues <- sum(is.na(actData$steps))
print (totMisValues)
## Devise a strategy for filling in all of the missing values in the dataset. The strategy does not need to be sophisticated. For example, you could use the mean/median for that day, or the mean for that 5-minute interval, etc
## The Strategy to input the missing values would be mean for that interval and below function returns mean value for that interval
getMissingValue <- function(interval) {
avgstepsperInterval[avgstepsperInterval$interval == interval,]$steps
}
## Create a new dataset that is equal to the original dataset but with the missing data filled in
refData <- actData
head(refData)
##for (i in 1:nrow(refData))
##{
##   if(is.na(refData[i,]$steps)) {
##     refData[i,]$steps <- getMissingValue(refData[i,]$interval)
##}
##}
summary(refData)
## Make a histogram of the total number of steps taken each day and Calculate and report the mean and median total number of steps taken per day. Do these values differ from the estimates from the first part of the assignment? What is the impact of imputing missing data on the estimates of the total daily number of steps?
s3 <- group_by(refData, date)
refStepsperday <- summarize(s3, ST = sum(steps))
print(refStepsperday)
## Load the Data
if(!file.exists('activity.csv')) {
unzip('activity.zip')
}
actData <- read.csv("activity.csv")
summary(actData)
## Preprocess the Data for formatting the date fields
actData$date = as.Date(actData$date)
## Loadneeded R packages
library(dplyr)
## Calculate the total number of steps taken per day
s1 <- group_by (actData, date)
stepsperday <- summarize(s1, stepsTotal = sum(steps, na.rm = TRUE))
print(stepsperday)
## If you do not understand the difference between a histogram and a barplot, research the difference between them. Make a histogram of the total number of steps taken each day
hist(stepsperday$stepsTotal, main = "Histogram of Steps Per Day", xlab = "Total Steps per day")
## Calculate and report the mean and median of the total number of steps taken per day
medianStepsperday <- median(stepsperday$stepsTotal)
meanStepsperday <- mean(stepsperday$stepsTotal)
## Median Steps per day
print(medianStepsperday)
## Mean Steps per day
print(meanStepsperday)
## Make a time series plot (i.e. type = "l") of the 5-minute interval (x-axis) and the average number of steps taken, averaged across all days (y-axis)
s2 <- group_by (actData, interval)
avgstepsperInterval <- summarize(s2, avgs = mean(steps, na.rm = TRUE))
print(avgstepsperInterval)
plot(avgstepsperInterval$interval, avgstepsperInterval$avgs, type = "l", main = "Average Daily Activity Pattern", xlab = "Intervals", ylab = "Average Steps")
##Which 5-minute interval, on average across all the days in the dataset, contains the maximum number of steps?
maxStepsPerInterval <-avgstepsperInterval[which.max(avgstepsperInterval$avgs),]$interval
print(maxStepsPerInterval)
## Calculate and report the total number of missing values in the dataset (i.e. the total number of rows with NAs)
totMisValues <- sum(is.na(actData$steps))
print (totMisValues)
## Devise a strategy for filling in all of the missing values in the dataset. The strategy does not need to be sophisticated. For example, you could use the mean/median for that day, or the mean for that 5-minute interval, etc
## The Strategy to input the missing values would be mean for that interval and below function returns mean value for that interval
getMissingValue <- function(interval) {
avgstepsperInterval[avgstepsperInterval$interval == interval,]$steps
}
## Create a new dataset that is equal to the original dataset but with the missing data filled in
refData <- actData
for (i in 1:nrow(refData))
{
if(is.na(refData[i,]$steps)) {
refData[i,]$steps <- getMissingValue(refData[i,]$interval)
}
}
